@inproceedings{werbrouck_ldac_2020,
  author    = {Werbrouck, Jeroen and Taelman, Ruben and Verborgh, Ruben and Pauwels, Pieter and Beetz, Jakob and Mannens, Erik},
  title     = {Pattern-based Access Control in a Decentralised Collaboration Environment},
  booktitle = {Proceedings of LDAC2020 - 8th Linked Data in Architecture and Construction Workshop},
  year      = {2020},
  month     = {june},
  url       = {https://www.researchgate.net/profile/Jeroen_Werbrouck/publication/342354922_Pattern-based_access_control_in_a_decentralised_collaboration_environment/links/5ef085dda6fdcc73be944024/Pattern-based-access-control-in-a-decentralised-collaboration-environment.pdf},
  abstract  = {
As the building industry is rapidly catching up with digital advancements, and Web technologies grow in both maturity and security , a data-and Web-based construction practice comes within reach. In such an environment, private project information and open online data can be combined to allow cross-domain interoperability at data level, using Semantic Web technologies. As construction projects often feature complex and temporary networks of stakeholder firms and their employees , a property-based access control mechanism is necessary to enable a flexible and automated management of distributed building projects. In this article, we propose a method to facilitate such mechanism using existing Web technologies: RDF, SHACL, WebIDs, nanopublications and the Linked Data Platform. The proposed method will be illustrated with an extension of a custom nodeJS Solid server. The potential of the Solid ecosystem has been put forward earlier as a basis for a Linked Data-based Common Data Environment: its decentralised setup, connection of both RDF and non-RDF resources and fine-grained access control mechanisms are considered an apt foundation to manage distributed building data.
  },
  _type     = {Conference}
}

@inproceedings{verborgh_amw_2020,
  author    = {Verborgh, Ruben and Taelman, Ruben},
  title     = {Guided Link-Traversal-Based Query Processing},
  month     = {may},
  year      = {2020},
  url       = {https://arxiv.org/abs/2005.02239},
  abstract  = {
Link-Traversal-Based Query Processing (LTBQP) is a technique for evaluating queries over a web of data by starting with a set of seed documents that is dynamically expanded through following hyperlinks. Compared to query evaluation over a static set of sources, LTBQP is significantly slower because of the number of needed network requests. Furthermore, there are concerns regarding relevance and trustworthiness of results, given that sources are selected dynamically. To address both issues, we propose guided LTBQP, a technique in which information about document linking structure and content policies is passed to a query processor. Thereby, the processor can prune the search tree of documents by only following relevant links, and restrict the result set to desired results by limiting which documents are considered for what kinds of content. In this exploratory paper, we describe the technique at a high level and sketch some of its applications. We argue that such guidance can make LTBQP a valuable query strategy in decentralized environments, where data is spread across documents with varying levels of user trust.
},
  _type     = {Preprint}
}

@article{taelman_phd_2020,
  author    = {Taelman, Ruben},
  title     = {Storing and Querying Evolving Knowledge Graphs on the Web},
  booktitle = {Storing and Querying Evolving Knowledge Graphs on the Web},
  year      = {2020},
  month     = {februari},
  url       = {https://phd.rubensworks.net/},
  abstract  = {
The Web has become our most valuable tool for sharing information. Currently, this Web is mainly targeted at humans, whereas machines typically have a hard time understanding information on the Web. Using knowledge graphs, this information can be linked in a structured way, so that intelligent agents can act upon this data autonomously. Current knowledge graphs remain however rather static. As there is a lot of value in acting upon evolving knowledge, there is a need for evolving knowledge graphs, and ways to manage them. As such, the goal of this PhD is to allow such evolving knowledge graphs to be stored and queried, taking into account the decentralized nature of the Web where anyone should be able to say anything about anything. Concretely, four challenges related to this goal are investigated: (1) generation of evolving data, (2) storage of evolving data, (3) querying over heterogeneous datasets, and (4) querying evolving data. For each of these challenges, techniques and algorithms have been developed, which prove to be valuable for storing and querying evolving knowledge graphs on the Web. This work therefore brings us closer towards a Web in which both human and machine can act upon evolving knowledge.

  },
  _type     = {PhD Thesis}
}

@article{buyle_egose_solid_2019,
  author    = {Buyle, Raf and Taelman, Ruben and Mostaert, Katrien and Joris, Geroen and Mannens, Erik and Verborgh, Ruben and Berners-Lee, Tim},
  title     = {Streamlining governmental processes by putting citizens in control of their personal data},
  booktitle = {Proceedings of the 6th International Conference on Electronic Governance and Open Society: Challenges in Eurasia},
  year      = {2019},
  month     = {november},
  url       = {https://drive.verborgh.org/publications/buyle_egose_2019.pdf},
  abstract  = {
Governments typically store large amounts of personal information on their citizens, such as a home address, marital status, and occupation, to offer public services. Because governments consist of various governmental agen- cies, multiple copies of this data often exist. This raises concerns regarding data consistency, privacy, and access control, especially under recent legal frame- works such as GDPR. To solve these problems, and to give citizens true control over their data, we explore an approach using the decentralised Solid ecosys- tem, which enables citizens to maintain their data in personal data pods. We have applied this approach to two high-impact use cases, where citizen infor- mation is stored in personal data pods, and both public and private organisations are selectively granted access. Our findings indicate that Solid allows reshaping the relationship between citizens, their personal data, and the applications they use in the public and private sector. We strongly believe that the insights from this Flemish Solid Pilot can speed up the process for public administrations and private organisations that want to put the users in control of their data.
  },
  _type     = {Conference}
}

@article{vandersande_semantics_heritage_2019,
  author    = {Vander Sande, Miel and de Valk, Sjors and Meijers, Enno and Taelman, Ruben and Van de Sompel, Herbert and Verborgh, Ruben},
  title     = {Discovering Data Sources in a Distributed Network of Heritage Information},
  booktitle = {Proceedings of the Posters and Demo Track of the 15th International Conference on Semantic Systems},
  year      = {2019},
  month     = {october},
  url       = {http://ceur-ws.org/Vol-2451/paper-28.pdf},
  abstract  = {
The Netwerk Digitaal Erfgoed is a Dutch partnership that focuses on improving the visibility, usability and sustainability of digital collections in the cultural heritage sector. The vision is to improve the usability of the data by sur-mounting the borders between the separate collections of the cultural heritage institutions. Key concepts in this vision are the alignment of the data by using shared descriptions (e.g. thesauri), and the publication of the data as Linked Open Data. This demo paper describes a Proof of Concept to test this vision. It uses a register, where only summaries of datasets are stored, instead of all the data. Based on these summaries, a portal can query the register to find what datasources might be of interest, and then query the data directly from the relevant data sources.
  },
  _type     = {Demo}
}

@article{taelman_iswc_ostrich_2019,
  author    = {Taelman, Ruben and Vander Sande, Miel and Van Herwegen, Joachim and Mannens, Erik and Verborgh, Ruben},
  title     = {Reflections on: Triple Storage for Random-Access
Versioned Querying of RDF Archives},
  booktitle = {Proceedings of the 18th International Semantic Web Conference},
  year      = {2019},
  month     = {october},
  url       = {https://rdfostrich.github.io/article-iswc2019-journal-ostrich/},
  abstract  = {
In addition to their latest version, Linked Open Datasets on the Web can also contain useful information in or between previous versions. In order to exploit this information, we can maintain history in RDF archives. Existing approaches either require much storage space, or they do not meet sufficiently expressive querying demands. In this extended abstract, we discuss an RDF archive indexing technique that has a low storage overhead, and adds metadata for reducing lookup times. We introduce algorithms based on this technique for efficiently evaluating versioned queries. Using the BEAR RDF archiving benchmark, we evaluate our implementation, called OSTRICH. Results show that OSTRICH introduces a new trade-off regarding storage space, ingestion time, and querying efficiency. By processing and storing more metadata during ingestion time, it significantly lowers the average lookup time for versioning queries. Our storage technique reduces query evaluation time through a preprocessing step during ingestion, which only in some cases increases storage space when compared to other approaches. This allows data owners to store and query multiple versions of their dataset efficiently, lowering the barrier to historical dataset publication and analysis.
  },
  _type     = {Conference},
  _slides   = {https://www.rubensworks.net/raw/slides/2019/iswc-ostrich/},
  note      = {Accepted for publication}
}

@inproceedings{vandevyvere_eswc_website_2019,
  author    = {Van de Vyvere, Brecht and Taelman, Ruben and Colpaert, Pieter and Verborgh, Ruben},
  title     = {Using an existing website as a queryable low-cost LOD publishing interface},
  booktitle = {Proceedings of the 16th Extended Semantic Web Conference: Posters and Demos},
  year      = {2019},
  month     = {june},
  url       = {https://brechtvdv.github.io/Article-Using-an-existing-website-as-a-queryable-low-cost-LOD-publishing-interface/},
  abstract  = {
Maintaining an Open Dataset comes at an extra recurring cost when it is published in a dedicated Web interface. As there is not often a direct financial return from publishing a dataset publicly, these extra costs need to be minimized. Therefore we want to explore reusing existing infrastructure by enriching existing websites with Linked Data. In this demonstrator, we advised the data owner to annotate a digital heritage website with JSON-LD snippets, resulting in a dataset of more than three million triples that is now available and officially maintained. The website itself is paged, and thus hydra partial collection view controls were added in the snippets. We then extended the modular query engine Comunica to support following page controls and extracting data from HTML documents while querying. This way, a SPARQL or GraphQL query over multiple heterogeneous data sources can power automated data reuse. While the query performance on such an interface is visibly poor, it becomes easy to create composite data dumps. As a result of implementing these building blocks in Comunica, any paged collection and enriched HTML page now becomes queryable by the query engine. This enables heterogenous data interfaces to share functionality and become technically interoperable.
  },
  _type     = {Demo}
}

@inproceedings{taelman_w3cdataws_graphql_2019,
  author    = {Taelman, Ruben and Vander Sande, Miel and Verborgh, Ruben},
  title     = {Bridges between GraphQL and RDF},
  booktitle = {W3C Workshop on Web Standardization for Graph Data},
  year      = {2019},
  month     = {march},
  url       = {https://rubensworks.github.io/article-w3cdataws2019-graphql/},
  abstract  = {
GraphQL offers a highly popular query languages for graphs, which is well known among Web developers. Each GraphQL data graph is scoped within an interface-specific schema, which makes it difficult to integrate data from multiple interfaces. The RDF graph model offers a solution to this integration problem. Different techniques can enable querying over RDF graphs using GraphQL queries. In this position statement, we provide a high-level overview of the existing techniques, and how they differ. We argue that each of these techniques have their merits, but standardization is essential to simplify the link between GraphQL and RDF.
  },
  _type     = {Position Statement},
  _slides   = {https://www.rubensworks.net/raw/slides/2019/w3c-data-ws-graphql-rdf/}
}

@article{taelman_jws_ostrich_2018,
  author    = {Taelman, Ruben and Vander Sande, Miel and Van Herwegen, Joachim and Mannens, Erik and Verborgh, Ruben},
  title     = {Triple Storage for Random-Access Versioned Querying of RDF Archives},
  journal   = {Journal of Web Semantics},
  year      = {2018},
  month     = {august},
  url       = {https://rdfostrich.github.io/article-jws2018-ostrich/},
  abstract  = {
When publishing Linked Open Datasets on the Web, most attention is typically directed to their latest version.
Nevertheless, useful information is present in or between previous versions.
In order to exploit this historical information in dataset analysis, we can maintain history in RDF archives.
Existing approaches either require much storage space, or they expose an insufficiently expressive or efficient interface with respect to querying demands.
In this article, we introduce an RDF archive indexing technique that is able to store datasets with a low storage overhead,
by compressing consecutive versions and adding metadata for reducing lookup times.
We introduce algorithms based on this technique for efficiently evaluating queries at a certain version, between any two versions, and for versions.
Using the BEAR RDF archiving benchmark, we evaluate our implementation, called OSTRICH.
Results show that OSTRICH introduces a new trade-off regarding storage space, ingestion time, and querying efficiency.
By processing and storing more metadata during ingestion time, it significantly lowers the average lookup time for versioning queries.
OSTRICH performs better for many smaller dataset versions than for few larger dataset versions.
Furthermore, it enables efficient offsets in query result streams, which facilitates random access in results.
Our storage technique reduces query evaluation time for versioned queries through a preprocessing step during ingestion,
which only in some cases increases storage space when compared to other approaches.
This allows data owners to store and query multiple versions of their dataset efficiently,
lowering the barrier to historical dataset publication and analysis.
  },
  _type     = {Journal},
  _highlighted = {true}
}

@article{taelman_swj_podigg_2018,
  author    = {Taelman, Ruben and Colpaert, Pieter and Mannens, Erik and Verborgh, Ruben},
  title     = {Generating Public Transport Data based on Population Distributions for RDF Benchmarking},
  journal   = {Semantic Web Journal},
  year      = {2018},
  month     = {july},
  url       = {http://rubensworks.net/raw/publications/2018/podigg.pdf},
  abstract  = {
When benchmarking RDF data management systems such as public transport route planners,
system evaluation needs to happen under various realistic circumstances,
which requires a wide range of datasets with different properties.
Real-world datasets are almost ideal, as they offer these realistic circumstances, but they are often hard to obtain and inflexible for testing.
For these reasons, synthetic dataset generators are typically preferred over real-world datasets due to their intrinsic flexibility.
Unfortunately, many synthetic dataset that are generated within benchmarks are insufficiently realistic,
raising questions about the generalizability of benchmark results to real-world scenarios.
In order to benchmark geospatial and temporal RDF data management systems such as route planners with sufficient external validity and depth,
we designed PoDiGG, a highly configurable generation algorithm for synthetic public transport datasets
with realistic geospatial and temporal characteristics comparable to those of their real-world variants.
The algorithm is inspired by real-world public transit network design and scheduling methodologies.
This article discusses the design and implementation of PoDiGG and validates the properties of its generated datasets.
Our findings show that the generator achieves a sufficient level of realism,
based on the existing coherence metric and new metrics we introduce specifically for the public transport domain.
Thereby, podigg provides a flexible foundation for benchmarking RDF data management systems with geospatial and temporal data.
  },
  _type     = {Journal},
  _highlighted = {true}
}

@inproceedings{taelman_iswc_resources_comunica_2018,
  author    = {Taelman, Ruben and Van Herwegen, Joachim and Vander Sande, Miel and Verborgh, Ruben},
  title     = {Comunica: a Modular SPARQL Query Engine for the Web},
  booktitle = {Proceedings of the 17th International Semantic Web Conference},
  year      = {2018},
  month     = {october},
  url       = {https://comunica.github.io/Article-ISWC2018-Resource/},
  abstract  = {
Query evaluation over Linked Data sources has become a complex story,
given the multitude of algorithms and techniques for single- and multi-source querying,
as well as the heterogeneity of Web interfaces through which data is published online.
Today’s query processors are insufficiently adaptable to test multiple query engine aspects in combination,
such as evaluating the performance of a certain join algorithm over a federation of heterogeneous interfaces.
The Semantic Web research community is in need of a flexible query engine that allows plugging in new components such as different algorithms,
new or experimental SPARQL features, and support for new Web interfaces.
We designed and developed a Web-friendly and modular meta query engine called Comunica that meets these specifications.
In this article, we introduce this query engine and explain the architectural choices behind its design.
We show how its modular nature makes it an ideal research platform for investigating new kinds of Linked Data interfaces and querying algorithms.
Comunica facilitates the development, testing, and evaluation of new query processing capabilities, both in isolation and in combination with others.
  },
  _type     = {Conference},
  _slides   = {https://www.rubensworks.net/raw/slides/2018/iswc-comunica/},
  _video    = {http://videolectures.net/iswc2018_taelman_comunica_modular_sparql/},
  _highlighted = {true}
}

@inproceedings{taelman_iswc_demo_graphqlld_2018,
  author    = {Taelman, Ruben and Vander Sande, Miel and Verborgh, Ruben},
  title     = {GraphQL-LD: Linked Data Querying with GraphQL},
  booktitle = {Proceedings of the 17th International Semantic Web Conference: Posters and Demos},
  year      = {2018},
  month     = {october},
  url       = {https://comunica.github.io/Article-ISWC2018-Demo-GraphQlLD/},
  abstract  = {
The Linked Open Data cloud has the potential of significantly enhancing and transforming end-user applications.
For example, the use of URIs to identify things allows data joining between separate data sources.
Most popular (Web) application frameworks, such as React and Angular have limited support for querying the Web of Linked Data,
which leads to a high-entry barrier for Web application developers.
Instead, these developers increasingly use the highly popular GraphQL query language for retrieving data from GraphQL APIs,
because GraphQL is tightly integrated into these frameworks.
In order to lower the barrier for developers towards Linked Data consumption, the Linked Open Data cloud needs to be queryable with GraphQL as well.
In this article, we introduce a method for transforming GraphQL queries coupled with a JSON-LD context to SPARQL,
and a method for converting SPARQL results to the GraphQL query-compatible response.
We demonstrate this method by implementing it into the Comunica framework.
This approach brings us one step closer towards widespread Linked Data consumption for application development.
  },
  _type     = {Demo},
  _poster   = {https://www.slideshare.net/RubenTaelman/poster-graphqlld-linked-data-querying-with-graphql}
}

@inproceedings{vanherwegen_iswc_demo_comunica_2018,
  author    = {Van Herwegen, Joachim and Taelman, Ruben and Vander Sande, Miel and Verborgh, Ruben},
  title     = {Demonstration of Comunica, a Web framework for querying heterogeneous Linked Data interfaces},
  booktitle = {Proceedings of the 17th International Semantic Web Conference: Posters and Demos},
  year      = {2018},
  month     = {october},
  url       = {https://comunica.github.io/Article-ISWC2018-Demo/},
  abstract  = {
Linked Data sources can appear in a variety of forms, going from SPARQL endpoints to Triple Pattern Fragments and data dumps.
This heterogeneity among Linked Data sources creates an added layer of complexity when querying or combining results from those sources.
To ease this problem, we created a modular engine, Comunica, that has modules for evaluating SPARQL queries and supports heterogeneous interfaces.
Other modules for other query or source types can easily be added.
In this paper we showcase a Web client that uses Comunica to evaluate federated SPARQL queries through automatic source type identification and interaction.
  },
  _type     = {Demo},
  _poster   = {https://www.slideshare.net/RubenTaelman/poster-demonstration-of-comunica-a-web-framework-for-querying-heterogeneous-linked-data-interfaces}
}

@inproceedings{taelman_iswc_workshop_semanticversionedquerying_2018,
  author    = {Taelman, Ruben and Takeda, Hideaki and Vander Sande, Miel and Verborgh, Ruben},
  title     = {The Fundamentals of Semantic Versioned Querying},
  booktitle = {Proceedings of the 12th International Workshop on Scalable Semantic Web Knowledge Base Systems
co-located with 17th International Semantic Web Conference},
  year      = {2018},
  month     = {october},
  url       = {https://rdfostrich.github.io/article-versioned-reasoning/},
  abstract  = {
The domain of RDF versioning concerns itself with the storage of different versions of Linked Datasets.
The ability of querying over these versions is an active area of research, and allows for basic insights to be discovered,
such as tracking the evolution of certain things in datasets. Querying can however only get you so far.
In order to derive logical consequences from existing knowledge, we need to be able to reason over this data, such as ontology-based inferencing.
In order to achieve this, we explore fundamental concepts on semantic querying of versioned datasets using ontological knowledge.
In this work, we present these concepts as a semantic extension of the existing RDF versioning concepts that focus on syntactical versioning.
We remain general and assume that versions do not necessarily follow a purely linear temporal relation.
This work lays a foundation for reasoning over RDF versions from a querying perspective,
using which RDF versioning storage, query and reasoning systems can be designed.
  },
  _type     = {Workshop},
  _slides   = {https://www.rubensworks.net/raw/slides/2018/iswc-semverquerying/}
}

@inproceedings{taelman_semantics_tpfqs_2018,
  author    = {Taelman, Ruben and Tommasini, Riccardo and Van Herwegen, Joachim and Vander Sande, Miel and Della Valle, Emanuele and Verborgh, Ruben},
  title     = {On the Semantics of TPF-QS towards Publishing and Querying RDF Streams at Web-scale},
  booktitle = {Proceedings of the 14th International Conference on Semantic Systems},
  year      = {2018},
  month     = {september},
  url       = {http://rubensworks.net/raw/publications/2018/on_the_semantics_of_tpf-qs_towards_publishing_and_querying_rdf_streams_at_web-scale.pdf},
  abstract  = {
RDF Stream Processing (RSP) is a rapidly evolving area of research that focuses on extensions of the Semantic Web in order to model and process Web data streams.
While state-of-the-art approaches concentrate on server-side processing of RDF streams,
we investigate the Triple Pattern Fragments Query Streamer (TPF-QS) method for server-side publishing of RDF streams,
which moves the workload of continuous querying to clients.
We formalize TPF-QS in terms of the RSP-QL reference model in order to formally compare it with existing RSP query languages.
We experimentally validate that, compared to the state of the art,
the server load of TPF-QS scales better with increasing numbers of concurrent clients in case of simple queries,
at the cost of increased bandwidth consumption.
This shows that TPF-QS is an important first step towards a viable solution for Web-scale publication and continuous processing of RDF streams.
},
  _type     = {Conference},
  _slides   = {https://www.rubensworks.net/raw/slides/2018/semantics-tpfqs/}
}

@inproceedings{taelman_eswc_challenge_2018,
  author    = {Taelman, Ruben and Vander Sande, Miel and Verborgh, Ruben},
  title     = {Versioned Querying with OSTRICH and Comunica in MOCHA 2018},
  booktitle = {Proceedings of the 5th SemWebEval Challenge at ESWC 2018},
  year      = {2018},
  month     = {may},
  url       = {https://rdfostrich.github.io/article-mocha-2018/},
  abstract  = {
In order to exploit the value of historical information in Linked Datasets,
we need to be able to store and query different versions of such datasets efficiently.
The 2018 edition of the Mighty Storage Challenge (MOCHA) is organized to discover the efficiency of such Linked Data stores and to detect their bottlenecks.
One task in this challenge focuses on the storage and querying of versioned datasets,
in which we participated by combining the OSTRICH triple store and the Comunica SPARQL engine.
In this article, we briefly introduce our system for the versioning task of this challenge.
We present the evaluation results that show that our system achieves fast query times for the supported queries,
but not all queries are supported by Comunica at the time of writing.
These results of this challenge will serve as a guideline for further improvements to our system.
  },
  _type     = {Challenge}
}

@inproceedings{taelman_www_dev_2018,
  author    = {Taelman, Ruben and Vander Sande, Miel and Verborgh, Ruben},
  title     = {Components.js: A Semantic Dependency Injection Framework},
  booktitle = {Proceedings of the The Web Conference: Developers Track},
  year      = {2018},
  month     = {april},
  url       = {http://componentsjs.readthedocs.io/},
  abstract  = {
Components.js is a dependency injection framework for JavaScript applications that allows components to be instantiated and wired together declaratively using semantic configuration files. 
The advantage of these semantic configuration files is that software components can be uniquely and globally identified using URIs. 
As an example, this documentation has been made self-instantiatable using Components.js. This makes it possible to view the HTML-version of any page to the console, or serve it via HTTP on a local webserver.
  },
  _type     = {Conference},
  _slides   = {https://www.slideshare.net/RubenTaelman/componentsjs}
}

@inproceedings{taelman_www_demo_2018,
  author    = {Taelman, Ruben and Vander Sande, Miel and Verborgh, Ruben},
  title     = {OSTRICH: Versioned Random-Access Triple Store},
  booktitle = {Proceedings of the 27th International Conference Companion on World Wide Web},
  year      = {2018},
  month     = {april},
  url       = {https://rdfostrich.github.io/article-demo/},
  abstract  = {
The Linked Open Data cloud is evergrowing and many datasets are frequently being updated.
In order to fully exploit the potential of the information that is available in and over historical dataset versions,
such as discovering evolution of taxonomies or diseases in biomedical datasets,
we need to be able to store and query the different versions of Linked Datasets efficiently.
In this demonstration, we introduce OSTRICH, which is an efficient triple store with supported for versioned query evaluation.
We demonstrate the capabilities of OSTRICH using a Web-based graphical user interface in which a store can be opened or created.
Using this interface, the user is able to query in, between, and over different versions, ingest new versions, and retrieve summarizing statistics.
  },
  _type     = {Demo},
  _poster   = {https://www.slideshare.net/RubenTaelman/poster-ostrich}
}

@inproceedings{andresrojas_www_2018,
  author    = {Rojas Mel{\'e}ndez, Juli\'{a}n Andr{\'e}s and Van de Vyvere, Brecht and Gevaert, Arne and Taelman, Ruben and Colpaert, Pieter and Verborgh, Ruben},
  title     = {A Preliminary Open Data Publishing Strategy for Live Data in Flanders},
  booktitle = {Proceedings of the 27th International Conference Companion on World Wide Web},
  year      = {2018},
  month     = {april},
  url       = {https://doi.org/10.1145/3184558.3191650},
  abstract  = {
For smart decision making, user agents need live and historic access to open data from sensors installed in the public domain.
In contrast to a closed environment, for Open Data and federated query processing algorithms,
the data publisher cannot anticipate in advance on specific questions,
nor can it deal with a bad cost-efficiency of the server interface when data consumers increase. When publishing observations from sensors,
different fragmentation strategies can be thought of depending on how the historic data needs to be queried.
Furthermore, both publish/subscribe and polling strategies exist to publish live updates.
Each of these strategies come with their own trade-offs regarding cost-efficiency of the server-interface, user-perceived performance and cpu use.
A polling strategy where multiple observations are published in a paged collection was tested in a proof of concept for parking spaces availability.
In order to understand the different resource trade-offs presented by publish/subscribe and polling publication strategies,
we devised an experiment on two machines, for a scalability test.
The preliminary results were inconclusive and suggest more large scale tests are needed in order to see a trend.
While the large-scale tests will be performed in future work,
the proof of concept helped to identify the technical Open Data principles for the 13 biggest cities in Flanders.
  },
  _type     = {Workshop}
}

@inproceedings{taelman_kcap_2017,
  title     = {Declaratively Describing Responses of Hypermedia-Driven {Web} {APIs}},
  author    = {Taelman, Ruben and Verborgh, Ruben},
  booktitle = {Proceedings of the 9th International Conference on Knowledge Capture},
  year      = {2017},
  month     = {dec},
  url       = {https://linkeddatafragments.github.io/Article-Declarative-Hypermedia-Responses/},
  abstract  = {
While humans browse the Web by following links, these hypermedia links can also be used by machines for browsing.
While efforts such as Hydra semantically describe the hypermedia controls on Web interfaces to enable smarter interface-agnostic clients,
they are largely limited to the input parameters to interfaces, and clients therefore do not know what response to expect from these interfaces.
In order to convey such expectations, interfaces need to declaratively describe the response structure of their parameterized hypermedia controls.
We therefore explored techniques to represent this parameterized response structure in a generic but expressive way.
In this work, we discuss four different approaches for declaring a response structure, and we compare them based on a model that we introduce.
Based on this model, we conclude that a SHACL shape-based approach can be used for declaring such a parameterized response structure,
as it conforms to the REST architectural style that has helped shape the Web into its current form.
  },
  _type     = {Conference},
  _poster   = {https://www.slideshare.net/RubenTaelman/poster-declaratively-describing-responses-of-hypermediadriven-web-apis}
}

@inproceedings{vanherwegen_semsci_2017,
  title     = {Describing configurations of software experiments as {Linked} {Data}},
  author    = {Van Herwegen, Joachim and Taelman, Ruben and Capadisli, Sarven and Verborgh, Ruben},
  booktitle = {Proceedings of the First Workshop on Enabling Open Semantic Science (SemSci)},
  year      = {2017},
  month     = {oct},
  url       = {https://linkedsoftwaredependencies.org/articles/describing-experiments/},
  abstract  = {
Within computer science engineering, research articles often rely on software experiments in order to evaluate contributions.
Reproducing such experiments involves setting up software, benchmarks, and test data.
Unfortunately, many articles ambiguously refer to software by name only, leaving out crucial details for reproducibility,
such as module and dependency version numbers or the configuration of individual components in different setups.
To address this, we created the Object-Oriented Components ontology for the semantic description of software components and their configuration.
This article discusses the ontology and its application, and demonstrates with a use case how to publish experiments and their software configurations on the Web.
In order to enable semantic interlinking between configurations and modules,
we published the metadata of all 500,000+ JavaScript libraries on npm as 200,000,000+ RDF triples.
Through our work, research articles can refer by URL to fine-grained descriptions of experimental setups.
This brings us faster to accurate reproductions of experiments, and facilitates the evaluation of new research contributions with different software configurations. 
In the future, software could be instantiated automatically based on these descriptions and configurations,
reasoning and querying can be applied to software configurations for meta-research purposes.
  },
  _type     = {Workshop},
  _slides   = {https://www.slideshare.net/JoachimVH/describing-configurations-of-software-experiments-as-linked-data}
}

@inproceedings{taelman_eswc_demo_2017,
  author    = {Taelman, Ruben and Vander Sande, Miel and Verborgh, Ruben and Mannens, Erik},
  title     = {Live Storage and Querying of Versioned Datasets on the {Web}},
  booktitle = {Proceedings of the 14th Extended Semantic Web Conference: Posters and Demos},
  year      = {2017},
  month     = {may},
  url       = {http://rubensworks.net/raw/publications/2017/vtpf-demo.pdf},
  abstract  = {
Linked Datasets often evolve over time for a variety of reasons.
While typical scenarios rely on the latest version only, useful knowledge may still be contained within or between older versions,
such as the historical information of biomedical patient data. In order to make this historical information cost- efficiently available on the Web,
a low-cost interface is required for providing access to versioned datasets.
For our demonstration, we set up a live Triple Pattern Fragments interface for a versioned dataset with queryable access.
We explain different version query types of this interface, and how it communicates with a storage solution that can handle these queries efficiently.
  },
  _type     = {Demo},
  _poster   = {https://www.slideshare.net/RubenTaelman/versioned-triple-pattern-fragments-76555249}
}

@inproceedings{taelman_mepdaw_2017,
  title     = {Versioned Triple Pattern Fragments: A Low-cost {Linked} {Data} Interface Feature for {Web} Archives},
  author    = {Taelman, Ruben and Vander Sande, Miel and Verborgh, Ruben and Mannens, Erik},
  booktitle = {Proceedings of the 3rd Workshop on Managing the Evolution and Preservation of the Data Web},
  year      = {2017},
  month     = {may},
  url       = {http://rubensworks.net/raw/publications/2017/vtpf.pdf},
  abstract  = {
Linked Datasets typically evolve over time because triples can be
removed from or added to datasets, which results in different dataset versions.
While most attention is typically given to the latest dataset version, a lot of useful
information is still present in previous versions and its historical evolution. In order
to make this historical information queryable at Web scale, a low-cost interface is
required that provides access to different dataset versions. In this paper, we add a
versioning feature to the existing Triple Pattern Fragments interface for queries at,
between and for versions, with an accompanying vocabulary for describing the
results, metadata and hypermedia controls. This interface feature is an important
step into the direction of making versioned datasets queryable on the Web, with a
low publication cost and effort.
  },
  _type     = {Workshop},
  _slides   = {https://www.slideshare.net/RubenTaelman/versioned-triple-pattern-fragments}
}

@inproceedings{taelman_www_poster_2017,
  author    = {Taelman, Ruben and Verborgh, Ruben and De Nies, Tom and Mannens, Erik},
  title     = {PoDiGG: A Public Transport RDF Dataset Generator},
  booktitle = {Proceedings of the 26th International Conference Companion on World Wide Web},
  year      = {2017},
  month     = {april},
  url       = {http://rubensworks.net/raw/publications/2017/PodiggPublicTransportRdfDatasetGenerator.pdf},
  abstract  = {
A large amount of public transport data is made available by many different providers,
which makes RDF a great method for integrating these datasets.
Furthermore, this type of data provides a great source of information that combines both geospatial and temporal data.
These aspects are currently undertested in RDF data management systems, because of the limited availability of realistic input datasets.
In order to bring public transport data to the world of benchmarking, we need to be able to create synthetic variants of this data.
In this paper, we introduce a dataset generator with the capability to create realistic public transport data.
This dataset generator, and the ability to configure it on different levels,
makes it easier to use public transport data for benchmarking with great flexibility.
  },
  _type     = {Poster},
  _poster   = {https://www.slideshare.net/RubenTaelman/podigg-public-transport-dataset-generator-based-on-population-distributions}
}

@Inbook{dimou_ekaw_workshop_2016,
  author    = {Dimou, Anastasia and Heyvaert, Pieter and Taelman, Ruben and Verborgh, Ruben},
  editor    = {Ciancarini, Paolo and Poggi, Francesco and Horridge, Matthew and Zhao, Jun and Groza, Tudor and Suarez-Figueroa, Mari Carmen and d'Aquin, Mathieu and Presutti, Valentina},
  title     = {Modeling, Generating, and Publishing Knowledge as Linked Data},
  bookTitle = {Knowledge Engineering and Knowledge Management: EKAW 2016 Satellite Events, EKM and Drift-an-LOD, Bologna, Italy, November 19--23, 2016, Revised Selected Papers},
  year      = {2017},
  publisher = {Springer International Publishing},
  address   = {Cham},
  pages     = {3--14},
  isbn      = {978-3-319-58694-6},
  doi       = {10.1007/978-3-319-58694-6_1},
  url       = {http://dx.doi.org/10.1007/978-3-319-58694-6_1},
  abstract  = {
The process of extracting, structuring, and organizing knowledge from one or multiple data sources
and preparing it for the Semantic Web requires a dedicated class of systems.
They enable processing large and originally heterogeneous data sources and capturing new knowledge.
Offering existing data as Linked Data increases its shareability, extensibility, and reusability.
However, using Linking Data as a means to represent knowledge can be easier said than done.
In this tutorial, we elaborate on the importance of semantically annotating data and how existing technologies facilitate their mapping to Linked Data.
We introduce [R2]RML languages to generate Linked Data derived from different heterogeneous data formats –e.g.,  DBs, XML, or JSON–
and from different interfaces –e.g.,  files or Web apis.
Those who are not Semantic Web experts can annotate their data with the RMLEditor,
whose user interface hides all underlying Semantic Web technologies to data owners.
Last, we show how to easily publish Linked Data on the Web as Triple Pattern Fragments.
As a result, participants, independently of their knowledge background, can model, annotate and publish data on their own.
  },
  _type     = {Tutorial}
}



@inproceedings{taelman_ekaw_poster_2016,
  author    = {Taelman, Ruben and Verborgh, Ruben and Mannens, Erik},
  title     = {Exposing RDF Archives using Triple Pattern Fragments},
  booktitle = {Proceedings of the 20th International Conference on Knowledge Engineering and Knowledge Management: Posters and Demos},
  year      = {2016},
  month     = {november},
  url       = {http://rubensworks.net/raw/publications/2016/ExposingRdfArchivesUsingTpf.pdf},
  abstract  = {
Linked Datasets typically change over time, and knowledge of this historical information can be useful.
This makes the storage and querying of Dynamic Linked Open Data an important area of research.
With the current versioning solutions, publishing Dynamic Linked Open Data at Web-Scale is possible, but too expensive.
We investigate the possibility of using the low-cost Triple Pattern Fragments (TPF) interface to publish versioned Linked Open Data.
In this paper, we discuss requirements for supporting versioning in the TPF framework, on the level of the interface, storage and client,
and investigate which trade-offs exist. These requirements lay the foundations for further research in the area of low-cost,
Web-Scale dynamic Linked Open Data publication and querying.
  },
  _type     = {Poster},
  _poster   = {https://www.slideshare.net/RubenTaelman/exposing-rdf-archives-using-triple-pattern-fragments}
}

@inproceedings{taelman_iswc_poster_2016,
  author    = {Taelman, Ruben and Heyvaert, Pieter and Verborgh, Ruben and Mannens, Erik},
  title     = {Querying Dynamic Datasources with Continuously Mapped Sensor Data},
  booktitle = {Proceedings of the 15th International Semantic Web Conference: Posters and Demos},
  year      = {2016},
  month     = {october},
  url       = {http://rubensworks.net/raw/publications/2016/QueryingDynamicDatasourcesWithContinuouslyMappedData.pdf},
  abstract  = {
The world contains a large amount of sensors that produce new data at
a high frequency. It is currently very hard to find public services that expose these
measurements as dynamic Linked Data. We investigate how sensor data can be
published continuously on the Web at a low cost. This paper describes how the
publication of various sensor data sources can be done by continuously mapping
raw sensor data to RDF and inserting it into a live, low-cost server. This makes it
possible for clients to continuously evaluate dynamic queries using public sensor
data. For our demonstration, we will illustrate how this pipeline works for the
publication of temperature and humidity data originating from a microcontroller,
and how it can be queried.
  },
  _type     = {Demo},
  _poster   = {http://www.slideshare.net/RubenTaelman/querying-dynamic-datasources-with-continuously-mapped-sensor-data}
}

@inproceedings{heyvaert_iswc_poster_2016,
  author    = {Heyvaert, Pieter and Taelman, Ruben and Verborgh, Ruben and Mannens, Erik},
  title     = {Linked Sensor Data Generation using Queryable RML Mappings},
  booktitle = {Proceedings of the 15th International Semantic Web Conference: Posters and Demos},
  year      = {2016},
  month     = {october},
  url       = {http://ceur-ws.org/Vol-1690/paper9.pdf},
  abstract  = {
As the amount of generated sensor data is increasing, semantic
interoperability becomes an important aspect in order to support
efficient data distribution and communication. Therefore, the integration
of (sensor) data is important, as this data is coming from different
data sources and might be in different formats. Furthermore, reusable
and extensible methods for this integration are required in order to be
able to scale with the growing number of applications that generate semantic
sensor data. Current research efforts allow to map sensor data
to Linked Data in order to provide semantic interoperability. However,
they lack support for multiple data sources, hampering the integration.
Furthermore, the used methods are not available for reuse or are not extensible,
which hampers the development of applications. In this paper,
we describe how the RDF Mapping Language (RML) and a Triple Pattern
Fragments (TPF) server are used to address these shortcomings. The
demonstration consists of a micro controller that generates sensor data.
The data is captured and mapped to rdf triples using module-specific
RML mappings, which are queried from a TPF server.
  },
  _type     = {Demo}
}

@inproceedings{taelman_iswc_cold_2016,
  author    = {Taelman, Ruben and Colpaert, Pieter and Verborgh, Ruben and Colpaert, Pieter and Mannens, Erik},
  title     = {Multidimensional Interfaces for Selecting Data within Ordinal Ranges},
  booktitle = {Proceedings of the 7th International Workshop on Consuming Linked Data},
  year      = {2016},
  month     = {october},
  url       = {http://rubensworks.net/raw/publications/2016/MultidimensionalInterfaces.pdf},
  abstract  = {
Linked Data interfaces exist in many flavours, as evidenced by subject
pages, sparql endpoints, triple pattern interfaces, and data dumps. These interfaces
are mostly used to retrieve parts of a complete dataset, such parts can for example be
defined by ranges in one or more dimensions. Filtering Linked Data by dimensions
such as time range, geospatial area, or genomic location, requires the lookup of data
within ordinal ranges. To make retrieval by such ranges generic and cost-efficient,
we propose a REST solution in-between looking up data within ordinal ranges
entirely on the server, or entirely on the client. To this end, we introduce a method
for extending any Linked Data interface with an n-dimensional interface-level index
such that n-dimensional ordinal data can be selected using n-dimensional ranges.
We formally define Range Gates and Range Fragments and theoretically evaluate
the cost-efficiency of hosting such an interface. By adding a multidimensional
index to a Linked Data interface for multidimensional ordinal data, we found that
we can get benefits from both worlds: the expressivity of the server raises, yet
remains more cost-efficient than an interface providing the full functionality on
the server-side. Furthermore, the client now shares in the effort to filter the data.
This makes query processing becomes more flexible to the end-user, because the
query plan can be altered by the engine. In future work we hope to apply Range
Gates and Range Fragments to real-world interfaces to give quicker access to data
within ordinal ranges
  },
  _type     = {Workshop},
  _slides   = {http://www.slideshare.net/RubenTaelman/multidimensional-interfaces-for-selecting-data-with-order}
}

@inproceedings{taelman_mepdaw_bp_2016,
  title        = {Continuous Client-Side Query Evaluation over Dynamic Linked Data},
  author       = {Taelman, Ruben and Verborgh, Ruben and Colpaert, Pieter and Mannens, Erik},
  booktitle    = {The Semantic Web: ESWC 2016 Satellite Events, Heraklion, Crete, Greece, May 29 -- June 2, 2016, Revised Selected Papers},
  pages        = {273--289},
  year         = {2016},
  month        = {may},
  organization = {Springer International Publishing},
  url          = {http://rubensworks.net/raw/publications/2016/Continuous_Client-Side_Query_Evaluation_over_Dynamic_Linked_Data.pdf},
  abstract     = {
Existing solutions to query dynamic Linked Data sources extend the sparql language, and require continuous server processing for each query.
Traditional sparql endpoints already accept highly expressive queries, so extending these endpoints for time-sensitive queries increases the server cost even further.
To make continuous querying over dynamic Linked Data more affordable, we extend the low-cost Triple Pattern Fragments (TPF) interface with support for time-sensitive queries.
In this paper, we introduce the TPF Query Streamer that allows clients to evaluate sparql queries with continuously updating results.
Our experiments indicate that this extension significantly lowers the server complexity, at the expense of an increase in the execution time per query.
We prove that by moving the complexity of continuously evaluating queries over dynamic Linked Data to the clients and thus increasing bandwidth usage, 
the cost at the server side is significantly reduced.
Our results show that this solution makes real-time querying more scalable for a large amount of concurrent clients when compared to the alternatives.
  },
  _type        = {Conference},
  _highlighted = {true}
}

@inproceedings{taelman_eswc_poster_2016,
  author    = {Taelman, Ruben and Verborgh, Ruben and Colpaert, Pieter and Mannens, Erik and Van de Walle, Rik},
  title     = {Moving Real-Time {Linked Data} Query Evaluation to the Client},
  booktitle = {Proceedings of the 13th Extended Semantic Web Conference: Posters and Demos},
  year      = {2016},
  month     = {may},
  url       = {http://2016.eswc-conferences.org/sites/default/files/papers/Accepted%20Posters%20and%20Demos/ESWC2016_POSTER_Moving_Real-Time_Linked_Data.pdf},
  abstract  = {
Traditional RDF stream processing engines work completely server-side, which contributes to a high server cost.
For allowing a large number of concurrent clients to do continuous querying,
we extend the low-cost Triple Pattern Fragments (TPF) interface with support for time-sensitive queries.
In this poster, we give the overview of a client-side RDF stream processing engine on top of TPF.
Our experiments show that our solution significantly lowers the server load while increasing the load on the clients.
Preliminary results indicate that our solution moves the complexity of continuously evaluating real-time queries
from the server to the client, which makes real-time querying much more scalable for a large amount of concurrent
clients when compared to the alternatives.
  },
  _type     = {Poster},
  _poster   = {http://www.slideshare.net/RubenTaelman/moving-rdf-stream-processing-to-the-client}
}

@inproceedings{taelman_eswc_phdsymp_2016,
  title        = {Continuously Self-Updating Query Results over Dynamic Heterogeneous Linked Data},
  author       = {Taelman, Ruben},
  booktitle    = {The Semantic Web. Latest Advances and New Domains: 13th International Conference, ESWC 2016, Heraklion, Crete, Greece, May 29 -- June 2, 2016, Proceedings},
  pages        = {863--872},
  year         = {2016},
  month        = {may},
  organization = {Springer International Publishing},
  url          = {http://rubensworks.net/raw/publications/2016/Continuously_Self-Updating_Query_Results_over_Dynamic_Heterogeneous_Linked_Data.pdf},
  abstract     = {
Our society is evolving towards massive data consumption from heterogeneous sources, which includes rapidly changing data
like public transit delay information.
Many applications that depend on dynamic data consumption require highly available server interfaces.
Existing interfaces involve substantial costs to publish rapidly changing data with high availability,
and are therefore only possible for
organisations that can afford such an expensive infrastructure.
In my doctoral research, I investigate how to publish and consume real-time and historical Linked Data on a large scale.
To reduce server-side costs for making dynamic data publication affordable,
I will examine different possibilities to divide query evaluation between servers and clients.
This paper discusses the methods I aim to follow together with preliminary results and the steps required to use this solution.
An initial prototype achieves significantly lower server processing cost per query, while maintaining reasonable
query execution times and client costs.
Given these promising results, I feel confident this research direction is a viable solution for offering low-cost
dynamic Linked Data interfaces as opposed to the existing high-cost solutions.
  },
  _type        = {PhD Symposium},
  _slides      = {http://www.slideshare.net/RubenTaelman/continuous-selfupdating-query-results-over-dynamic-linked-data},
  _poster      = {http://www.slideshare.net/RubenTaelman/scalable-dynamic-data-consumption-on-the-web}
}

@inproceedings{taelman_mepdaw_2016,
  title     = {Continuously Updating Query Results over Real-Time {Linked Data}},
  author    = {Taelman, Ruben and Verborgh, Ruben and Colpaert, Pieter and Mannens, Erik and Van de Walle, Rik},
  booktitle = {Proceedings of the 2nd Workshop on Managing the Evolution and Preservation of the Data Web},
  year      = {2016},
  month     = {may},
  url       = {http://ceur-ws.org/Vol-1585/mepdaw2016_paper_01.pdf},
  abstract  = {
Existing solutions to query dynamic Linked Data sources extend the SPARQL language,
and require continuous server processing for each query.
Traditional SPARQL endpoints accept highly expressive queries, contributing to high server cost.
Extending these endpoints for time-sensitive queries increases the server cost even further.
To make continuous querying over real-time Linked Data more affordable,
we extend the low-cost Triple Pattern Fragments (TPF) interface with support for time-sensitive queries.
In this paper, we discuss a framework on top of TPF that allows clients to execute
SPARQL queries with continuously updating results.
Our experiments indicate that this extension significantly lowers the server complexity.
The trade-off is an increase in the execution time per query.
We prove that by moving the complexity of continuously evaluating real-time queries over Linked Data to the clients
and thus increasing the bandwidth usage, the cost of server-side interfaces is significantly reduced.
Our results show that this solution makes real-time querying more scalable in terms of cpu usage for a large amount
of concurrent clients when compared to the alternatives.
  },
  _type     = {Workshop},
  _slides   = {http://www.slideshare.net/RubenTaelman/continuously-updating-query-results-over-realtime-linked-data}
}

@MastersThesis{taelman_mastersthesis,
  author   = {Taelman, Ruben},
  title    = {Continuously Updating Queries over Real-Time {Linked} {Data}},
  school   = {Ghent University},
  address  = {Belgium},
  year     = {2015},
  month    = {may},
  url      = {http://lib.ugent.be/fulltxt/RUG01/002/224/537/RUG01-002224537_2015_0001_AC.pdf},
  abstract = {
This dissertation investigates the possibilities of having continuously updating
queries over Linked Data with a focus on server availability. This work builds upon the ideas
of Linked Data Fragments to let the clients do most of the work when executing a query. The
server adds metadata to make the clients aware of the data volatility for making sure the
query results are always up-to-date. The implementation of the framework that is proposed,
is eventually tested and compared to other alternative approaches.
  },
  _type    = {Master's Thesis}
}
